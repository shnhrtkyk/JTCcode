{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shnhrtkyk/JTCcode/blob/main/05_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AB%E3%82%88%E3%82%8B%E7%82%B9%E7%BE%A4%E3%81%AE%E5%88%86%E9%A1%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1miMjSd91R"
      },
      "source": [
        "# 点群の分類を行う深層学習手法"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、入力された点群がどのようなクラスであるかを予測する点群の分類タスクを行います。例えば、ロボットの目の前にある物体を計測した点群がなんのクラスなのかがわかれば、ロボットの行動を最適に行えます。"
      ],
      "metadata": {
        "id": "QiuRcm753WoT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSjF8zBBd5Cx"
      },
      "source": [
        "## 点群に対する深層学習手法のライブラリインストール"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このノートブックではPytorchと、Pytorch向けの3次元点群処理用ライブラリであるPytorch Geometricを用います。"
      ],
      "metadata": {
        "id": "KnA3pN2B1Uxa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6omvItcSd0FZ"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install tensorboardX\n",
        "\n",
        "# Helper functions for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 可視化に関する関数を作成\n",
        "def visualize_mesh(pos, face):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    ax.axes.xaxis.set_ticklabels([])\n",
        "    ax.axes.yaxis.set_ticklabels([])\n",
        "    ax.axes.zaxis.set_ticklabels([])\n",
        "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=data.face.t(), antialiased=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_points(pos, edge_index=None, index=None):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    if edge_index is not None:\n",
        "        for (src, dst) in edge_index.t().tolist():\n",
        "             src = pos[src].tolist()\n",
        "             dst = pos[dst].tolist()\n",
        "             plt.plot([src[0], dst[0]], [src[1], dst[1]], linewidth=1, color='black')\n",
        "    if index is None:\n",
        "        plt.scatter(pos[:, 0], pos[:, 1], s=50, zorder=1000)\n",
        "    else:\n",
        "       mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
        "       mask[index] = True\n",
        "       plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000)\n",
        "       plt.scatter(pos[mask, 0], pos[mask, 1], s=50, zorder=1000)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLbp5piOeFx_"
      },
      "source": [
        "## データをダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "点群の分類タスク向けの公開データセットであるModelNetを用います。ModelNetは家具や家電などの3Dモデルから仮想的に作成した点群のデータで、点群とそれに対応するクラス情報が紐づいています。ModelNetには分類するクラス数が違うデータセットがあり、ここでは簡単な10クラス分類のデータセットを用います。\n",
        "※ダウンロードに時間がかかります。"
      ],
      "metadata": {
        "id": "B01AkSif3znh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LejEjt7Idil5"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from torch_geometric.datasets import ModelNet\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "current_path = Path.cwd()\n",
        "dataset_dir = current_path / \"modelnet10\"\n",
        "\n",
        "pre_transform = T.Compose([\n",
        "    T.SamplePoints(1024, remove_faces=True, include_normals=True),\n",
        "    T.NormalizeScale(),\n",
        "])\n",
        "# name で使用するModelNetの種類を選択する。\n",
        "train_dataset = ModelNet(dataset_dir, name=\"10\", train=True, transform=None, pre_transform=pre_transform, pre_filter=None)\n",
        "test_dataset = ModelNet(dataset_dir, name=\"10\", train=False, transform=None, pre_transform=pre_transform, pre_filter=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分類するクラスは以下のようにIDが降られています。"
      ],
      "metadata": {
        "id": "zMF0H_qz8Z8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path(\"/content/modelnet10/raw\")\n",
        "folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n",
        "classes = {folder: i for i, folder in enumerate(folders)}\n",
        "classes"
      ],
      "metadata": {
        "id": "VEESACSE70tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkLElgjA5aiB"
      },
      "source": [
        "### データの確認"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットとして読み込んだModelNetから点群を取得して可視化します。\n",
        "今回は訓練データの1000番目の点群を可視化してみます。"
      ],
      "metadata": {
        "id": "ZsmXMf7n8nIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww8Ebgpw5bDQ"
      },
      "outputs": [],
      "source": [
        "tmp =  train_dataset[1000]\n",
        "print(tmp.y)\n",
        "point = tmp.pos\n",
        "x = point[:, 0]\n",
        "y = point[:, 1]\n",
        "z = point[:, 2]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練データの数を確認します。"
      ],
      "metadata": {
        "id": "_B78FamB81aw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw29j54Fdil7"
      },
      "outputs": [],
      "source": [
        "print(\"train_dataset len:\", len(train_dataset))\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch Geometricで読み込んだ点群データの扱い方について説明します。\n",
        "`.pos`がXYZ座標を保持しています。 N点×XYZ座標の行列です。\n"
      ],
      "metadata": {
        "id": "7oF0T16d84wI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brptd_wwdil8"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0].pos.shape)\n",
        "print(train_dataset[0].pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に、PytorchGeometricのデータ読み込みについて説明します。\n",
        "深層学習のモデルを学習する際には、バッチと呼ばれる小さなデータ単位で処理を行います。今回はバッチサイズを32として、一回の学習に32個の点群を呼び出します。\n",
        "\n"
      ],
      "metadata": {
        "id": "OIBlDU1U9Vq5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gBtZFW9dil8"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader as DataLoader\n",
        "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "batch = next(iter(dataloader))\n",
        "print(batch)\n",
        "print(batch.pos)# 座標\n",
        "print(batch.y) # 真値ラベル"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PointNetの層設計"
      ],
      "metadata": {
        "id": "8sm1sApDJ9qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 層設計"
      ],
      "metadata": {
        "id": "XoPp0bVmKJCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### シンメトリック関数"
      ],
      "metadata": {
        "id": "mP0KUZb6KJCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず、入力された点群の順番に対する依存を抜くためのシンメトリック関数を実装します。ここでは、シンメトリック関数としてMaxPoolingを使用します。"
      ],
      "metadata": {
        "id": "1k52T2yHWBhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjDBJWekKJCq"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import global_max_pool\n",
        "import torch.nn as nn\n",
        "\n",
        "class SymmFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SymmFunction, self).__init__()\n",
        "        self.shared_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 512),\n",
        "        )\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        x = self.shared_mlp(batch.pos)\n",
        "        x = global_max_pool(x, batch.batch)\n",
        "        return x\n",
        "\n",
        "f = SymmFunction()\n",
        "print(batch)\n",
        "y = f(batch)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TNet"
      ],
      "metadata": {
        "id": "1lBHhn9PKJCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TNetは入力された点群の剛体変形を吸収するために開発された関数です。\n"
      ],
      "metadata": {
        "id": "4tUY-5MXkeC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_rg4pb1KJCq"
      },
      "outputs": [],
      "source": [
        "class InputTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InputTNet, self).__init__()\n",
        "        self.input_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
        "        )\n",
        "        self.output_mlp = nn.Sequential(\n",
        "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            nn.Linear(256, 9)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, batch):\n",
        "        x = self.input_mlp(x)\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = self.output_mlp(x)\n",
        "        x = x.view(-1, 3, 3)\n",
        "        id_matrix = torch.eye(3).to(x.device).view(1, 3, 3).repeat(x.shape[0], 1, 1)\n",
        "        x = id_matrix + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TNet(特徴量)"
      ],
      "metadata": {
        "id": "pwAM9ny2KJCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TNetは点群の幾何的な変形だけでなく、特徴量空間でも適用することで剛体変形の影響を削減します。\n"
      ],
      "metadata": {
        "id": "L2jEuF0Bklle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UntR2ur1KJCr"
      },
      "outputs": [],
      "source": [
        "class FeatureTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureTNet, self).__init__()\n",
        "        self.input_mlp = nn.Sequential(\n",
        "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
        "        )\n",
        "        self.output_mlp = nn.Sequential(\n",
        "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            nn.Linear(256, 64*64)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, batch):\n",
        "        x = self.input_mlp(x)\n",
        "        x = global_max_pool(x, batch)\n",
        "        x = self.output_mlp(x)\n",
        "        x = x.view(-1, 64, 64)\n",
        "        id_matrix = torch.eye(64).to(x.device).view(1, 64, 64).repeat(x.shape[0], 1, 1)\n",
        "        x = id_matrix + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PointNetの本体"
      ],
      "metadata": {
        "id": "c4QfxJ0f6aJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNetは非常に単純な構造を持っています。\n",
        "入力された点群の各点に対して、MLPを適用し、その後にシンメトリック関数であるマックスプーリングによって入力された点群の順番に関する依存を抜きます。\n",
        "また、TNetを入力された点群に適用し、さらに特徴量空間でも適用します。します。"
      ],
      "metadata": {
        "id": "YFIxaSzHktXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Q0JtJedil-"
      },
      "outputs": [],
      "source": [
        "class PointNetClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointNetClassification, self).__init__()\n",
        "        # TNet\n",
        "        self.input_tnet = InputTNet()\n",
        "        # 各点に適用するMLP\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "        )\n",
        "        self.feature_tnet = FeatureTNet()\n",
        "        # 各点に適用するMLP\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Linear(64, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Linear(128, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
        "        )\n",
        "        # 各点に適用するMLP\n",
        "        self.mlp3 = nn.Sequential(\n",
        "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(p=0.3),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(p=0.3),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "        \n",
        "    def forward(self, batch_data):\n",
        "        x = batch_data.pos\n",
        "        # まずTNetで変換する\n",
        "        input_transform = self.input_tnet(x, batch_data.batch)\n",
        "        transform = input_transform[batch_data.batch, :, :]\n",
        "        x = torch.bmm(transform, x.view(-1, 3, 1)).view(-1, 3)\n",
        "\n",
        "        # 各点に適用するMLP\n",
        "        x = self.mlp1(x)\n",
        "        \n",
        "        feature_transform = self.feature_tnet(x, batch_data.batch)\n",
        "        transform = feature_transform[batch_data.batch, :, :]\n",
        "        x = torch.bmm(transform, x.view(-1, 64, 1)).view(-1, 64)\n",
        "\n",
        "        x = self.mlp2(x)        \n",
        "\n",
        "        # シンメトリック関数であるマックスプーリングで順番の依存を抜く\n",
        "        x = global_max_pool(x, batch_data.batch)\n",
        "        x = self.mlp3(x)\n",
        "        \n",
        "        return x, input_transform, feature_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PointNetの訓練"
      ],
      "metadata": {
        "id": "dQAnwgBflfeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの呼び出しなど"
      ],
      "metadata": {
        "id": "da9r9iBelfeL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs7I3cvKlfeL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "num_epoch = 3\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = PointNetClassification()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epoch // 4, gamma=0.5)\n",
        "\n",
        "log_dir = current_path / \"log_modelnet10_classification_pointnet\"\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "criteria = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練コード"
      ],
      "metadata": {
        "id": "mJv_-YAllfeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oTMoH6wlfeM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(num_epoch)):\n",
        "    model = model.train()\n",
        "    \n",
        "    losses = []\n",
        "    for batch_data in tqdm(train_dataloader, total=len(train_dataloader)):\n",
        "        batch_data = batch_data.to(device)\n",
        "        this_batch_size = batch_data.batch.detach().max() + 1\n",
        "        \n",
        "        pred_y, _, feature_transform = model(batch_data)\n",
        "        true_y = batch_data.y.detach()\n",
        "\n",
        "        class_loss = criteria(pred_y, true_y)\n",
        "        accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
        "\n",
        "        id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
        "        transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
        "        reg_loss = transform_norm.mean()\n",
        "\n",
        "        loss = class_loss + reg_loss * 0.001\n",
        "        \n",
        "        losses.append({\n",
        "            \"loss\": loss.item(),\n",
        "            \"class_loss\": class_loss.item(),\n",
        "            \"reg_loss\": reg_loss.item(),\n",
        "            \"accuracy\": accuracy,\n",
        "            \"seen\": float(this_batch_size)})\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        \n",
        "    if (epoch % 1 == 0):\n",
        "        model_path = log_dir / f\"model_{epoch:06}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    loss = 0\n",
        "    class_loss = 0\n",
        "    reg_loss = 0\n",
        "    accuracy = 0\n",
        "    seen = 0\n",
        "    for d in losses:\n",
        "        seen = seen + d[\"seen\"]\n",
        "        loss = loss + d[\"loss\"] * d[\"seen\"]\n",
        "        class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
        "        reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
        "        accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
        "    loss = loss / seen\n",
        "    class_loss = class_loss / seen\n",
        "    reg_loss = reg_loss / seen\n",
        "    accuracy = accuracy / seen\n",
        "    writer.add_scalar(\"train_epoch/loss\", loss, epoch)\n",
        "    writer.add_scalar(\"train_epoch/class_loss\", class_loss, epoch)\n",
        "    writer.add_scalar(\"train_epoch/reg_loss\", reg_loss, epoch)\n",
        "    writer.add_scalar(\"train_epoch/accuracy\", accuracy, epoch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "\n",
        "        losses = []\n",
        "        for batch_data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "            batch_data = batch_data.to(device)\n",
        "            this_batch_size = batch_data.batch.detach().max() + 1\n",
        "\n",
        "            pred_y, _, feature_transform = model(batch_data)\n",
        "            true_y = batch_data.y.detach()\n",
        "\n",
        "            class_loss = criteria(pred_y, true_y)\n",
        "            accuracy =float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
        "\n",
        "            id_matrix = torch.eye(feature_transform.shape[1]).to(feature_transform.device).view(1, 64, 64).repeat(feature_transform.shape[0], 1, 1)\n",
        "            transform_norm = torch.norm(torch.bmm(feature_transform, feature_transform.transpose(1, 2)) - id_matrix, dim=(1, 2))\n",
        "            reg_loss = transform_norm.mean()\n",
        "\n",
        "            loss = class_loss + reg_loss * 0.001\n",
        "\n",
        "            losses.append({\n",
        "                \"loss\": loss.item(),\n",
        "                \"class_loss\": class_loss.item(),\n",
        "                \"reg_loss\": reg_loss.item(),\n",
        "                \"accuracy\": accuracy,\n",
        "                \"seen\": float(this_batch_size)})\n",
        "            \n",
        "        loss = 0\n",
        "        class_loss = 0\n",
        "        reg_loss = 0\n",
        "        accuracy = 0\n",
        "        seen = 0\n",
        "        for d in losses:\n",
        "            seen = seen + d[\"seen\"]\n",
        "            loss = loss + d[\"loss\"] * d[\"seen\"]\n",
        "            class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
        "            reg_loss = reg_loss + d[\"reg_loss\"] * d[\"seen\"]\n",
        "            accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
        "        loss = loss / seen\n",
        "        class_loss = class_loss / seen\n",
        "        reg_loss = reg_loss / seen\n",
        "        accuracy = accuracy / seen\n",
        "        writer.add_scalar(\"test_epoch/loss\", loss, epoch)\n",
        "        writer.add_scalar(\"test_epoch/class_loss\", class_loss, epoch)\n",
        "        writer.add_scalar(\"test_epoch/reg_loss\", reg_loss, epoch)\n",
        "        writer.add_scalar(\"test_epoch/accuracy\", accuracy, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63xCTDAF6JoQ"
      },
      "source": [
        "## PointNet++の層設計"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここからは、深層学習モデルの構築に移ります。PytorchGeometricを用いて実際にPointNet++を実装してみます。"
      ],
      "metadata": {
        "id": "2faNlfyb9_uY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kOowuH56Rdo"
      },
      "source": [
        "### サブサンプリング関数"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNet++では、サブサンプリングを階層的に行いながら特徴量の算出を行います。\n",
        "Pytorch Geometricでは、最遠点サンプリング（FPSsampling）が関数として定義されています。\n",
        "PointNet++では、FPSによサブサンプリングする点の選択、選択された点の周辺にある点を探す処理、周辺点の特徴量を選択された点へ集約する、の流れで処理を行います。"
      ],
      "metadata": {
        "id": "2SLxQ6s_-KkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDNIPFSEdil8"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
        "class SAModule(torch.nn.Module):\n",
        "    def __init__(self, ratio, r, nn):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio # サブサンプリングする割合\n",
        "        self.r = r # グルーピングする際に使用する半径情報\n",
        "        self.conv = PointNetConv(nn, add_self_loops=False) # PointNetの関数を使用する\n",
        "\n",
        "    def forward(self, x, pos, batch):\n",
        "        idx = fps(pos, batch, ratio=self.ratio) # FPSによるサブサンプリングを行い代表点の決定\n",
        "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
        "                          max_num_neighbors=64) # 各代表点に入る半径rに入る点の探索\n",
        "        edge_index = torch.stack([col, row], dim=0)\n",
        "        x_dst = None if x is None else x[idx]\n",
        "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index) # PointNetによる特徴抽出\n",
        "        pos, batch = pos[idx], batch[idx]\n",
        "        return x, pos, batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-1wU50r6Uxn"
      },
      "source": [
        "### グローバルサブサンプリング"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "グローバルサンプリングでは、各点で算出した特徴量を、ひとつのベクトルへ集約することでクラス分類が実行できます。"
      ],
      "metadata": {
        "id": "wmgFVvXh-9TQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKjoGlTXdil9"
      },
      "outputs": [],
      "source": [
        "# クラスとして定義\n",
        "class GlobalSAModule(torch.nn.Module):\n",
        "    def __init__(self, nn):\n",
        "        super().__init__()\n",
        "        self.nn = nn # 引数で渡された特徴抽出を行う層を適用する\n",
        "\n",
        "    def forward(self, x, pos, batch):\n",
        "        x = self.nn(torch.cat([x, pos], dim=1))\n",
        "        x = global_max_pool(x, batch) # 入力された点群の特徴に対してグローバル特徴を抽出\n",
        "        pos = pos.new_zeros((x.size(0), 3))\n",
        "        batch = torch.arange(x.size(0), device=batch.device)\n",
        "        return x, pos, batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlyTmYKb6WGp"
      },
      "source": [
        "### PointNet++の本体"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記で定義したサブサンプリングを繰り返し適用します。  \n",
        "SAModuleの引数には、サブサンプリングする際の割合、グルーピングの半径、特徴抽出を行う際のチャンネル数を渡します。\n",
        "最初の層では、入力点群が持つXYZ座標なので、MLPの入力チャンネル数は３から始まります。\n"
      ],
      "metadata": {
        "id": "rhBLpl19_Qo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y0uOAH-dil9"
      },
      "outputs": [],
      "source": [
        "class PointNet2Classification(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 特徴抽出を行う\n",
        "        self.sa1_module = SAModule(0.5, 0.2, MLP([3, 64, 64, 128])) # サブサンプリング1回目\n",
        "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))# サブサンプリング2回目\n",
        "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024])) #点群全体の特徴を拾うためにグローバル特徴を得る\n",
        "\n",
        "        self.mlp = MLP([1024, 512, 256, 10], dropout=0.5, norm=None) # 単純なMLPで分類を行う\n",
        "\n",
        "    def forward(self, data):\n",
        "        sa0_out = (data.x, data.pos, data.batch)\n",
        "        sa1_out = self.sa1_module(*sa0_out)\n",
        "        sa2_out = self.sa2_module(*sa1_out)\n",
        "        sa3_out = self.sa3_module(*sa2_out)\n",
        "        x, pos, batch = sa3_out\n",
        "\n",
        "        return self.mlp(x).log_softmax(dim=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTtQvaUL6eKh"
      },
      "source": [
        "## PointNet++の訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKD9-20k6fm3"
      },
      "source": [
        "### モデルの呼び出しなど"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2n23EJ7dil-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "num_epoch = 3 # 今回は3エポック回します。\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# モデルを呼びだす\n",
        "model = PointNet2Classification()\n",
        "model = model.to(device)\n",
        "# 学習に関する設定を行う\n",
        "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epoch // 4, gamma=0.5)\n",
        "# ログを出す\n",
        "log_dir = current_path / \"log_modelnet10_classification_pointnet2\"\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "# データの設定\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "# 損失関数はクロスエントロピー\n",
        "criteria = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDpuw26t6iTY"
      },
      "source": [
        "### 訓練コード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8TRItTSdil_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(num_epoch)):\n",
        "    model = model.train()\n",
        "    \n",
        "    losses = []\n",
        "    for batch_data in tqdm(train_dataloader, total=len(train_dataloader)):\n",
        "        batch_data = batch_data.to(device)\n",
        "        this_batch_size = batch_data.batch.detach().max() + 1\n",
        "        \n",
        "        pred_y = model(batch_data)\n",
        "        true_y = batch_data.y.detach()\n",
        "\n",
        "        class_loss = criteria(pred_y, true_y)\n",
        "        accuracy = float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
        "\n",
        "        loss = class_loss \n",
        "        \n",
        "        losses.append({\n",
        "            \"loss\": loss.item(),\n",
        "            \"class_loss\": class_loss.item(),\n",
        "            \"accuracy\": accuracy,\n",
        "            \"seen\": float(this_batch_size)})\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "        \n",
        "    if (epoch % 1 == 0):\n",
        "        model_path = log_dir / f\"model_{epoch:06}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    loss = 0\n",
        "    class_loss = 0\n",
        "    accuracy = 0\n",
        "    seen = 0\n",
        "    for d in losses:\n",
        "        seen = seen + d[\"seen\"]\n",
        "        loss = loss + d[\"loss\"] * d[\"seen\"]\n",
        "        class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
        "        accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
        "    loss = loss / seen\n",
        "    class_loss = class_loss / seen\n",
        "    accuracy = accuracy / seen\n",
        "    writer.add_scalar(\"train_epoch/loss\", loss, epoch)\n",
        "    writer.add_scalar(\"train_epoch/class_loss\", class_loss, epoch)\n",
        "    writer.add_scalar(\"train_epoch/accuracy\", accuracy, epoch)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "\n",
        "        losses = []\n",
        "        for batch_data in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "            batch_data = batch_data.to(device)\n",
        "            this_batch_size = batch_data.batch.detach().max() + 1\n",
        "\n",
        "            pred_y = model(batch_data)\n",
        "            true_y = batch_data.y.detach()\n",
        "\n",
        "            class_loss = criteria(pred_y, true_y)\n",
        "            accuracy =float((pred_y.argmax(dim=1) == true_y).sum()) / float(this_batch_size)\n",
        "\n",
        "\n",
        "\n",
        "            loss = class_loss \n",
        "\n",
        "            losses.append({\n",
        "                \"loss\": loss.item(),\n",
        "                \"class_loss\": class_loss.item(),\n",
        "                \"accuracy\": accuracy,\n",
        "                \"seen\": float(this_batch_size)})\n",
        "            \n",
        "        loss = 0\n",
        "        class_loss = 0\n",
        "        accuracy = 0\n",
        "        seen = 0\n",
        "        for d in losses:\n",
        "            seen = seen + d[\"seen\"]\n",
        "            loss = loss + d[\"loss\"] * d[\"seen\"]\n",
        "            class_loss = class_loss + d[\"class_loss\"] * d[\"seen\"]\n",
        "            accuracy = accuracy + d[\"accuracy\"] * d[\"seen\"]\n",
        "        loss = loss / seen\n",
        "        class_loss = class_loss / seen\n",
        "        accuracy = accuracy / seen\n",
        "        writer.add_scalar(\"test_epoch/loss\", loss, epoch)\n",
        "        writer.add_scalar(\"test_epoch/class_loss\", class_loss, epoch)\n",
        "        writer.add_scalar(\"test_epoch/accuracy\", accuracy, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 結果の比較"
      ],
      "metadata": {
        "id": "30mY8JqssQ6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNetの結果"
      ],
      "metadata": {
        "id": "LU3eU3UdtjFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        " \n",
        " \n",
        "#TensorBoard起動（表示したいログディレクトリを指定）\n",
        "%tensorboard --logdir=\"/content/log_modelnet10_classification_pointnet\" --port 6006"
      ],
      "metadata": {
        "id": "MXWdz-2YszW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNet++の結果"
      ],
      "metadata": {
        "id": "r8P7L7LHtvCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4UoXxbldil_"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        " \n",
        " \n",
        "#TensorBoard起動（表示したいログディレクトリを指定）\n",
        "%tensorboard --logdir=\"/content/log_modelnet10_classification_pointnet2\"  --port 6007"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i -P | grep 6006"
      ],
      "metadata": {
        "id": "N6kW0KbwtWwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 "
      ],
      "metadata": {
        "id": "HWVnyZD_tjUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PointNet++をライブラリを使わずに書く"
      ],
      "metadata": {
        "id": "d_o7LNkb9mM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "def timeit(tag, t):\n",
        "    print(\"{}: {}s\".format(tag, time() - t))\n",
        "    return time()\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    l = pc.shape[0]\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc\n",
        "# 距離を計算\n",
        "def square_distance(src, dst):\n",
        "    \"\"\"\n",
        "    Calculate Euclid distance between each two points.\n",
        "\n",
        "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
        "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
        "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
        "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
        "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
        "\n",
        "    Input:\n",
        "        src: source points, [B, N, C]\n",
        "        dst: target points, [B, M, C]\n",
        "    Output:\n",
        "        dist: per-point square distance, [B, N, M]\n",
        "    \"\"\"\n",
        "    B, N, _ = src.shape\n",
        "    _, M, _ = dst.shape\n",
        "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
        "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
        "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
        "    return dist\n",
        "\n",
        "\n",
        "def index_points(points, idx):\n",
        "    \"\"\"\n",
        "\n",
        "    Input:\n",
        "        points: input points data, [B, N, C]\n",
        "        idx: sample index data, [B, S]\n",
        "    Return:\n",
        "        new_points:, indexed points data, [B, S, C]\n",
        "    \"\"\"\n",
        "    device = points.device\n",
        "    B = points.shape[0]\n",
        "    view_shape = list(idx.shape)\n",
        "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "    repeat_shape = list(idx.shape)\n",
        "    repeat_shape[0] = 1\n",
        "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
        "    new_points = points[batch_indices, idx, :]\n",
        "    return new_points\n",
        "\n",
        "# サブサンプリングする関数\n",
        "def farthest_point_sample(xyz, npoint):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: pointcloud data, [B, N, 3]\n",
        "        npoint: number of samples\n",
        "    Return:\n",
        "        centroids: sampled pointcloud index, [B, npoint]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
        "    distance = torch.ones(B, N).to(device) * 1e10\n",
        "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
        "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
        "    for i in range(npoint):\n",
        "        centroids[:, i] = farthest\n",
        "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
        "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
        "        mask = dist < distance\n",
        "        distance[mask] = dist[mask]\n",
        "        farthest = torch.max(distance, -1)[1]\n",
        "    return centroids\n",
        "\n",
        "# グルーピングする関数\n",
        "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        radius: local region radius\n",
        "        nsample: max sample number in local region\n",
        "        xyz: all points, [B, N, 3]\n",
        "        new_xyz: query points, [B, S, 3]\n",
        "    Return:\n",
        "        group_idx: grouped points index, [B, S, nsample]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    _, S, _ = new_xyz.shape\n",
        "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
        "    sqrdists = square_distance(new_xyz, xyz)\n",
        "    group_idx[sqrdists > radius ** 2] = N\n",
        "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
        "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
        "    mask = group_idx == N\n",
        "    group_idx[mask] = group_first[mask]\n",
        "    return group_idx\n",
        "\n",
        "# FPSとグルーピングをまとめた関数\n",
        "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        npoint:\n",
        "        radius:\n",
        "        nsample:\n",
        "        xyz: input points position data, [B, N, 3]\n",
        "        points: input points data, [B, N, D]\n",
        "    Return:\n",
        "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
        "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
        "    \"\"\"\n",
        "    B, N, C = xyz.shape\n",
        "    S = npoint\n",
        "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
        "    new_xyz = index_points(xyz, fps_idx)\n",
        "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
        "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
        "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
        "\n",
        "    if points is not None:\n",
        "        grouped_points = index_points(points, idx)\n",
        "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
        "    else:\n",
        "        new_points = grouped_xyz_norm\n",
        "    if returnfps:\n",
        "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
        "    else:\n",
        "        return new_xyz, new_points\n",
        "\n",
        "# FPSとグルーピングをまとめた関数\n",
        "def sample_and_group_all(xyz, points):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: input points position data, [B, N, 3]\n",
        "        points: input points data, [B, N, D]\n",
        "    Return:\n",
        "        new_xyz: sampled points position data, [B, 1, 3]\n",
        "        new_points: sampled points data, [B, 1, N, 3+D]\n",
        "    \"\"\"\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
        "    grouped_xyz = xyz.view(B, 1, N, C)\n",
        "    if points is not None:\n",
        "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
        "    else:\n",
        "        new_points = grouped_xyz\n",
        "    return new_xyz, new_points\n",
        "\n",
        "# 特徴抽出を行うクラス\n",
        "class PointNetSetAbstraction(nn.Module):\n",
        "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
        "        super(PointNetSetAbstraction, self).__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius = radius\n",
        "        self.nsample = nsample\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = in_channel\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
        "            last_channel = out_channel\n",
        "        self.group_all = group_all\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            xyz: input points position data, [B, C, N]\n",
        "            points: input points data, [B, D, N]\n",
        "        Return:\n",
        "            new_xyz: sampled points position data, [B, C, S]\n",
        "            new_points_concat: sample points feature data, [B, D', S]\n",
        "        \"\"\"\n",
        "        xyz = xyz.permute(0, 2, 1)\n",
        "        if points is not None:\n",
        "            points = points.permute(0, 2, 1)\n",
        "\n",
        "        if self.group_all:\n",
        "            new_xyz, new_points = sample_and_group_all(xyz, points) # サブサンプリングとグルーピング\n",
        "        else:\n",
        "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
        "        # new_xyz: sampled points position data, [B, npoint, C]\n",
        "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
        "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            bn = self.mlp_bns[i]\n",
        "            new_points =  F.relu(bn(conv(new_points))) # MLPを適用\n",
        "\n",
        "        new_points = torch.max(new_points, 2)[0] #MaxPoolingで代表値を決める\n",
        "        new_xyz = new_xyz.permute(0, 2, 1)\n",
        "        return new_xyz, new_points\n",
        "\n",
        "\n",
        "class PointNetSetAbstractionMsg(nn.Module):\n",
        "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
        "        super(PointNetSetAbstractionMsg, self).__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius_list = radius_list\n",
        "        self.nsample_list = nsample_list\n",
        "        self.conv_blocks = nn.ModuleList()\n",
        "        self.bn_blocks = nn.ModuleList()\n",
        "        for i in range(len(mlp_list)):\n",
        "            convs = nn.ModuleList()\n",
        "            bns = nn.ModuleList()\n",
        "            last_channel = in_channel + 3\n",
        "            for out_channel in mlp_list[i]:\n",
        "                convs.append(nn.Conv2d(last_channel, out_channel, 1)) \n",
        "                bns.append(nn.BatchNorm2d(out_channel))\n",
        "                last_channel = out_channel\n",
        "            self.conv_blocks.append(convs)\n",
        "            self.bn_blocks.append(bns)\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            xyz: input points position data, [B, C, N]\n",
        "            points: input points data, [B, D, N]\n",
        "        Return:\n",
        "            new_xyz: sampled points position data, [B, C, S]\n",
        "            new_points_concat: sample points feature data, [B, D', S]\n",
        "        \"\"\"\n",
        "        xyz = xyz.permute(0, 2, 1)\n",
        "        if points is not None:\n",
        "            points = points.permute(0, 2, 1)\n",
        "\n",
        "        B, N, C = xyz.shape\n",
        "        S = self.npoint\n",
        "        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n",
        "        new_points_list = []\n",
        "        for i, radius in enumerate(self.radius_list):\n",
        "            K = self.nsample_list[i]\n",
        "            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n",
        "            grouped_xyz = index_points(xyz, group_idx)\n",
        "            grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
        "            if points is not None:\n",
        "                grouped_points = index_points(points, group_idx)\n",
        "                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n",
        "            else:\n",
        "                grouped_points = grouped_xyz\n",
        "\n",
        "            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n",
        "            for j in range(len(self.conv_blocks[i])):\n",
        "                conv = self.conv_blocks[i][j]\n",
        "                bn = self.bn_blocks[i][j]\n",
        "                grouped_points =  F.relu(bn(conv(grouped_points)))\n",
        "            new_points = torch.max(grouped_points, 2)[0]  # [B, D', S]\n",
        "            new_points_list.append(new_points)\n",
        "\n",
        "        new_xyz = new_xyz.permute(0, 2, 1)\n",
        "        new_points_concat = torch.cat(new_points_list, dim=1)\n",
        "        return new_xyz, new_points_concat\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i1mv8xB_9l5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class get_model(nn.Module):\n",
        "    def __init__(self,num_class,normal_channel=True):\n",
        "        super(get_model, self).__init__()\n",
        "        in_channel = 6 if normal_channel else 3\n",
        "        self.normal_channel = normal_channel\n",
        "        self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False)\n",
        "        self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False)\n",
        "        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.drop1 = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.drop2 = nn.Dropout(0.4)\n",
        "        self.fc3 = nn.Linear(256, num_class)\n",
        "\n",
        "    def forward(self, xyz):\n",
        "        B, _, _ = xyz.shape\n",
        "        if self.normal_channel:\n",
        "            norm = xyz[:, 3:, :]\n",
        "            xyz = xyz[:, :3, :]\n",
        "        else:\n",
        "            norm = None\n",
        "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
        "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
        "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
        "        x = l3_points.view(B, 1024)\n",
        "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, -1)\n",
        "\n",
        "\n",
        "        return x, l3_points\n",
        "\n",
        "\n",
        "\n",
        "class get_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(get_loss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target, trans_feat):\n",
        "        total_loss = F.nll_loss(pred, target)\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "wIj9esIM98FT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}