{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNesCm1fRHiPheoShznb5/8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shnhrtkyk/JTCcode/blob/main/05_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 物体検出"
      ],
      "metadata": {
        "id": "GLTG3ywON_7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "点群の物体検出は、点群中に存在する物体の位置・サイズ・向きを推定するタスクです。\n",
        "物体検出の使われる状況は、自動運転の歩行者検出や車の検出であり、リアルタイムな処理が必要です。\n"
      ],
      "metadata": {
        "id": "cmsX0uDYOCE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 環境設定"
      ],
      "metadata": {
        "id": "OAtv9-gBOVOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -d /usr/local/cuda-*\n",
        "!which nvcc"
      ],
      "metadata": {
        "id": "otD7KhkenZY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDAの設定"
      ],
      "metadata": {
        "id": "cTdmxyHOOXZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "p = os.getenv('PATH')\n",
        "ld = os.getenv('LD_LIBRARY_PATH')\n",
        "os.environ['PATH'] = f\"/usr/local/cuda-11.8/bin:{p}\"\n",
        "os.environ['LD_LIBRARY_PATH'] = f\"/usr/local/cuda-11.8/lib64:{ld}\"\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "kOi7FXgHnVEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリのインストール"
      ],
      "metadata": {
        "id": "NsUKVRmeOdNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDAが認識されているかを確認する"
      ],
      "metadata": {
        "id": "h4BLBbWRhba2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Ynb5JFZyj4Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open3d"
      ],
      "metadata": {
        "id": "OYwJif5rgSTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open3Dの機械学習機能をインストール"
      ],
      "metadata": {
        "id": "IQ_fc06SOnB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open3Dには、機械学習、特に深層学習手法の学習済みモデルが公開されています。"
      ],
      "metadata": {
        "id": "nMnh0_NxOrvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/isl-org/Open3D-ML.git"
      ],
      "metadata": {
        "id": "kPhNBON_hcSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open3Dの機械学習機能は自分でビルドする必要があります。"
      ],
      "metadata": {
        "id": "ZEs8y5IDhjnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず、ディレクトリへ移動します。"
      ],
      "metadata": {
        "id": "Z3vYWKOXO3hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Open3D-ML"
      ],
      "metadata": {
        "id": "wi1CezhahkMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要ライブラリを確認する。"
      ],
      "metadata": {
        "id": "D-X_CjOgPEtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements-torch-cuda.txt"
      ],
      "metadata": {
        "id": "Ds21HzVrhv1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "必要なライブラリをインストールする。"
      ],
      "metadata": {
        "id": "jXrYTkJdPIqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111\n",
        "!pip install -r requirements-torch-cuda.txt\n",
        "!pip install -r requirements-tensorflow.txt"
      ],
      "metadata": {
        "id": "X66BBBR7hpkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習済みモデルによる推論"
      ],
      "metadata": {
        "id": "SbSZp8IbPvZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、深層学習による物体検出手法であるpointpillarsの学習済みモデルを用います。\n",
        "pointpillarsは、pointnetを物体検出向けに拡張した手法です。\n",
        "\n",
        "#### 概要\n",
        "一般的に，点群のみ使用したネットワークは精度が高いです．しかし低速という問題があります．一方，点群を画像に投影するネットワークは高速だが精度が低いという欠点があった。\n",
        "そこで，Point Pillarsは点群の細かい情報量を失わないように情報量をエンコードし、疑似画像に変換する。その疑似画像を2D CNNで使用するような物体検出ネットワークに入力し、物体検出を行う。 \n",
        "この手法の進歩性は従来単純に点群を俯瞰画像といった疑似画像に投影し物体検出CNNに入力するだけでは点群の細かい情報量が失われてしまっていた。そこで点群を画像に投影するためのエンコードネットワークを用いることで点群情報を失わずに物体検出CNN（SSD)に入力データを与え高精度化を達成した。\n",
        "具体的に点群を画像にエンコードするために、Pillar(柱）と呼ぶ点群を細かく格子状に分割しPointNetのような点群DNNを使いPillar内の特徴量を抽出。そして得た2Dの特徴量マップをSSDに与えることで物体検出を行う。 \n",
        "アイデア自体は非常にシンプルながら、PointNetと物体検出CNNに結合する手法です。\n",
        "\n",
        "#### 層設計\n",
        "VoxelNetではVoxel単位でPointNetを用いて特徴量を算出してから画像に投影し物体検出を行いました．\n",
        "Point Pillarsでは，以下の手順でEnd-to-Endに物体検出を行います．\n",
        "1.\tPillar Feature Net: 点群をスパースなpseudo imageに変換する\n",
        "2.\tBackbone (2D CNN): pseudo imageをハイレベル特徴量へ変換する\n",
        "3.\tDetection Head(SSD): 検出と３Dボックスへの回帰を行う\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "この３つの手順の中で重要なのが「Pillar Feature Net」なので，詳細に説明します．\n",
        "まず，点群をx, y平面でPillarを用いて分割します．\n",
        "この際，Pillar内部に入る点に対してpillar内の点の平均値からの差とpillar中心からの距離を計算し，新たな特徴量とします．\n",
        "次に，Pillar内の点群に対してPointNetを適用して特徴量を計算します．\n",
        "作成した特徴量を2次元画像に投影することで，擬似画像を作成します．\n",
        "この後に，通常の画像に対する物体検出手法を適用すれば，車や歩行者等の物体が検出できます．\n",
        " \n",
        "\n",
        "#### ピラーの作成方法\n",
        "入力次元では，絶対座標（x, y, z）, 反射強度rの4チャンネルの他に，ピーラー内部の相対的な座標情報（ピラー内の平均値からの差 (x_c, y_c, z_c)とピラー中心からの距離(x_p, y_p)）を作成し，9次元とします．\n",
        "ピラーの集合はDxPxNの次元を持つ．\n",
        "P: 点の入ったピラーの数（空っぽのピラーは無視する）\n",
        "N: ピラー内の点の数\n",
        "D: 入力次元であれば9，途中の層であればチャンネル数\n",
        "※ピラー内部にN点以上あった場合は，ランダムにサンプリングしてN点抽出する．逆に，N点より少ない場合は，Nになるように残りをゼロ埋めする．なお，入力順番はNの階乗だけバリエーションがありますが，後段の処理で順番依存が抜けるので気にしないで大丈夫です．\n",
        "\n",
        "これは，1x1の畳み込みを適用すると，Nの方向には独立しつつD方向に全結合のMLPと等価となり，高速な演算が可能になります．\n",
        "\n",
        "ピラー単位の特徴抽出によってCxPxNのテンソルができたとします．なお，C\n",
        "は特徴マップがCチャンネルの意味です．ここから，ピラー内の代表値を推定します．代表値にはピラー内に存在するN点に対する特徴マップのMaxPoolingによって得られた値を用います．ピラーの代表値ができため，CxPのテンソルができます．\n",
        "この後は鳥瞰図の画像に投影します．この時は，ピラーのIDとxy座標の関係を保持した情報を元にグリッド画像（CxHxW）に並べなおします．HとWは画像の高さと幅です．\n",
        "\n",
        "#### 物体に応じたネットワーク構造が必要\n",
        "ピラーを作成する際に，物体の大きさによって物体検出を行う際のバックボーンネットワークのストライドのサイズを変更する必要がある．例えば，歩行者と自転車は小さなストライドで抽出できるが，車を抽出するには大きなストライドが必要です．小さなストライドでは車の特徴を作成できず，逆に大きなストライドでは歩行者の微細な特徴を消してしまう可能性があります．\n",
        "\n",
        "#### データ拡張\n",
        "１．\tランダムに配置\n",
        "もともと観測された点群だけでなく，ランダムに物体を配置します．\n",
        "２．\t物体単位の拡張\n",
        "ランダムに各物体をx,y,z方向に-9~９度回転させたり，x,y,z座標をシフトさせたりします．\n",
        "３．\t全体的な拡張\n",
        "道路の傾きをデータ拡張で得るために，点群全体に傾き加えます．また，x,y,z座標に全体的なシフトをかけます．\n",
        "\n",
        "#### loss\n",
        "Loss設計は物体のある座標(x,y,z)，ボックスの大きさ（w, l, h），回転(θ)の値を回帰で最小化します．各 x,y,z, w,l,h, θの真値と推定結果の差は以下のように定義します．なお，サイズによるlossへの寄与度を正規化するため，サイズ由来のパラメータで割る処理を入れています．\n",
        " \n",
        " \n",
        " \n",
        "\n",
        "なお，Smooth L1 lossは通常のL1距離よりも，真値に近い場合には誤差の値を小さくするような機能を持ちます．定義は以下のようになります．\n",
        " \n",
        "\n",
        "この際，回転角の回帰は0 and πラジアンで計算するため，前を向いているのか後ろを向いているのかの区別つかないような現象が発生します．そこで，どの向きなのかを分類するloss(L_dir)を加えています．（著者の実装では，前か後ろの二値分類）\n",
        "\n",
        "また，物体検出は物体のある場所だけでなく，クラスも推定する必要があるため，画像の物体検出で用いられるFocal lossを計算します．\n",
        " \n",
        "\n",
        "最終的なlossはこれらのlossを線形に足したものです．なお，N_posは物体検出されたアンカーの数です．βは各lossを混ぜる際のパラメータで人間が設定します．\n",
        "\n",
        " \n",
        "\n",
        "#### 評価手法\n",
        "画像の物体検出と同様に，Average Precision(AP)を用います．mAPはクラスで平均した値です．なお，IoUは0.5に設定されています．\n",
        "\n",
        "#### Slimmer Design.について\n",
        "Point Pillarでは層設計を軽量化することで高速化を図りました．\n",
        "先行研究のVoxelnetではPointNetを二回適用しているため処理速度が遅いという問題がありました．Point Pillarでは，PointNetの適用を一回にすることで，Voxelnetより2.5 ms高速になりました．\n",
        "また，PointNet内の次元数を削減し4.5 ms高速化し，Detectionを担う層の次元数を下げて3.9 ms高速化しました．\n",
        "\n",
        "#### 実装上の高速化\n",
        "Pytorchで実装したが，NVIDIA TensorRT(GPU推論に最適化されたライブラリ) を使用することで，より最適化され，ピュアなPytorchよりも45.5%高速化できた．\n",
        "JETSONを使用したエッジ側での推論が可能になる可能性がある．\n"
      ],
      "metadata": {
        "id": "rfnabDPyP29g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "物体検出を行うデータは、数１０ＧＢあるので、この演習では扱いません、以下に学習済みモデルを呼び出すサンプルコードを置きますので、参考にしてください。\n",
        "\n",
        "` /path/to/your/dataset`をダウンロードした場所に指定すると、動きます。\n"
      ],
      "metadata": {
        "id": "TSSKujFxRfi4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NalsL-d7gLz3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import open3d.ml as _ml3d\n",
        "import open3d.ml.tf  as ml3d\n",
        "\n",
        "cfg_file = \"ml3d/configs/pointpillars_kitti.yml\"\n",
        "cfg = _ml3d.utils.Config.load_from_file(cfg_file)\n",
        "\n",
        "model = ml3d.models.PointPillars(**cfg.model)\n",
        "cfg.dataset['dataset_path'] = \"/path/to/your/dataset\"\n",
        "dataset = ml3d.datasets.KITTI(cfg.dataset.pop('dataset_path', None), **cfg.dataset)\n",
        "pipeline = ml3d.pipelines.ObjectDetection(model, dataset=dataset, device=\"gpu\", **cfg.pipeline)\n",
        "\n",
        "# download the weights.\n",
        "ckpt_folder = \"./logs/\"\n",
        "os.makedirs(ckpt_folder, exist_ok=True)\n",
        "ckpt_path = ckpt_folder + \"pointpillars_kitti_202012221652utc.pth\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/Open3D-ML/logs\"\n",
        "!wget \"https://storage.googleapis.com/open3d-releases/model-zoo/pointpillars_kitti_202012221652utc.pth\"\n",
        "!ls \n",
        "\n"
      ],
      "metadata": {
        "id": "W0XV4GUhUDLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the parameters.\n",
        "pipeline.load_ckpt(ckpt_path=ckpt_path)\n",
        "\n",
        "test_split = dataset.get_split(\"test\")\n",
        "data = test_split.get_data(0)\n",
        "\n",
        "# run inference on a single example.\n",
        "# returns dict with 'predict_labels' and 'predict_scores'.\n",
        "result = pipeline.run_inference(data)\n",
        "\n",
        "# evaluate performance on the test set; this will write logs to './logs'.\n",
        "pipeline.run_test()"
      ],
      "metadata": {
        "id": "EirrmNznUFmx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}