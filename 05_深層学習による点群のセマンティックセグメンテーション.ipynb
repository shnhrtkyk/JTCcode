{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shnhrtkyk/JTCcode/blob/main/05_%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AB%E3%82%88%E3%82%8B%E7%82%B9%E7%BE%A4%E3%81%AE%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E3%82%BB%E3%82%B0%E3%83%A1%E3%83%B3%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbbe526-23ee-4a90-af8c-9f7092f50192",
      "metadata": {
        "id": "cdbbe526-23ee-4a90-af8c-9f7092f50192"
      },
      "source": [
        "# セマンティックセグメンテーションの実装"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "セマンティックセグメンテーションとは、入力された各点にクラス情報を付与する手法です。\n",
        "セマンティックセグメンテーションにより、入力された点群がどのような環境なのか、具体的には道路の上に車があるなどの環境認識を3次元空間で行うことができます。"
      ],
      "metadata": {
        "id": "7ExH1iONn2dj"
      },
      "id": "7ExH1iONn2dj"
    },
    {
      "cell_type": "markdown",
      "id": "ed5c4e28-f40c-4c6a-827a-e28ff6db0501",
      "metadata": {
        "id": "ed5c4e28-f40c-4c6a-827a-e28ff6db0501"
      },
      "source": [
        "# 必要なライブラリをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726d3107-dd0a-440a-a164-43b708af858f",
      "metadata": {
        "id": "726d3107-dd0a-440a-a164-43b708af858f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebab99e-383f-410d-93a0-c8ff29dad2de",
      "metadata": {
        "id": "4ebab99e-383f-410d-93a0-c8ff29dad2de"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec996ce-98b9-465d-b530-e0f88ac7080d",
      "metadata": {
        "id": "dec996ce-98b9-465d-b530-e0f88ac7080d"
      },
      "source": [
        "## ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f2a4cd-24e6-457d-b7f6-7fc94912fd12",
      "metadata": {
        "id": "c7f2a4cd-24e6-457d-b7f6-7fc94912fd12"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_scatter import scatter\n",
        "from torchmetrics.classification import MulticlassJaccardIndex\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ShapeNet\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MLP, DynamicEdgeConv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71deb4e-f171-4a31-9801-a0e8f9cc9c04",
      "metadata": {
        "id": "c71deb4e-f171-4a31-9801-a0e8f9cc9c04"
      },
      "source": [
        "## 初期化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1053dec-59c9-4e9d-84e2-a1297dbf715f",
      "metadata": {
        "id": "a1053dec-59c9-4e9d-84e2-a1297dbf715f"
      },
      "outputs": [],
      "source": [
        "config_seed = 42\n",
        "config_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "random.seed(config_seed)\n",
        "torch.manual_seed(config_seed)\n",
        "device = torch.device(config_device)\n",
        "\n",
        "# 対象のクラスを選ぶ、今回はギターのセグメンテーションをします。\n",
        "config_category = \"Guitar\" # [\"Bag\", \"Cap\", \"Car\", \"Chair\", \"Earphone\", \"Guitar\", \"Knife\", \"Lamp\", \"Laptop\", \"Motorbike\", \"Mug\", \"Pistol\", \"Rocket\", \"Skateboard\", \"Table\"] \n",
        "config_random_jitter_translation = 1e-2\n",
        "config_random_rotation_interval_x = 15\n",
        "config_random_rotation_interval_y = 15\n",
        "config_random_rotation_interval_z = 15\n",
        "config_validation_split = 0.2\n",
        "config_batch_size = 16\n",
        "config_num_workers = 6\n",
        "\n",
        "config_num_nearest_neighbours = 30\n",
        "config_aggregation_operator = \"max\"\n",
        "config_dropout = 0.5\n",
        "config_initial_lr = 1e-3\n",
        "config_lr_scheduler_step_size = 5\n",
        "config_gamma = 0.8\n",
        "# エポック数は1回です\n",
        "config_epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a72e20-ff1b-4cdf-b87f-10baff51bac7",
      "metadata": {
        "id": "57a72e20-ff1b-4cdf-b87f-10baff51bac7"
      },
      "source": [
        "# ShapeNetというデータセットを読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ShapeNetという、物体のパーツごとにクラスが付与されたデータセットを用います。\n",
        "※ダウンロードに時間がかかります。"
      ],
      "metadata": {
        "id": "NB5HcG6UwnAw"
      },
      "id": "NB5HcG6UwnAw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42955441-03d8-4e9f-b856-7d7edc708981",
      "metadata": {
        "id": "42955441-03d8-4e9f-b856-7d7edc708981"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomJitter(config_random_jitter_translation),\n",
        "    T.RandomRotate(config_random_rotation_interval_x, axis=0),\n",
        "    T.RandomRotate(config_random_rotation_interval_y, axis=1),\n",
        "    T.RandomRotate(config_random_rotation_interval_z, axis=2)\n",
        "])\n",
        "pre_transform = T.NormalizeScale()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da27bf0f-1e0f-438d-b602-2124606e7138",
      "metadata": {
        "id": "da27bf0f-1e0f-438d-b602-2124606e7138"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join('ShapeNet', config_category)\n",
        "\n",
        "train_val_dataset = ShapeNet(\n",
        "    dataset_path, config_category, split='trainval',\n",
        "    transform=transform, pre_transform=pre_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e335648-933d-4865-aaaf-0f24c0358d16",
      "metadata": {
        "id": "3e335648-933d-4865-aaaf-0f24c0358d16"
      },
      "source": [
        "セグメンテーションデータの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e14c4af-b5af-4805-a479-f0725d6ef1de",
      "metadata": {
        "id": "9e14c4af-b5af-4805-a479-f0725d6ef1de"
      },
      "outputs": [],
      "source": [
        "segmentation_class_frequency = {}\n",
        "for idx in tqdm(range(len(train_val_dataset))):\n",
        "    pc_viz = train_val_dataset[idx].pos.numpy().tolist()\n",
        "    segmentation_label = train_val_dataset[idx].y.numpy().tolist()\n",
        "    for label in set(segmentation_label):\n",
        "        segmentation_class_frequency[label] = segmentation_label.count(label)\n",
        "class_offset = min(list(segmentation_class_frequency.keys()))\n",
        "print(\"Class Offset:\", class_offset)\n",
        "\n",
        "for idx in range(len(train_val_dataset)):\n",
        "    train_val_dataset[idx].y -= class_offset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練データと検証データへの切り分け"
      ],
      "metadata": {
        "id": "tGOZGdcJw3B4"
      },
      "id": "tGOZGdcJw3B4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10e09ff7-e64b-4912-a30b-461d5eb72cb2",
      "metadata": {
        "id": "10e09ff7-e64b-4912-a30b-461d5eb72cb2"
      },
      "outputs": [],
      "source": [
        "num_train_examples = int((1 - config_validation_split) * len(train_val_dataset))\n",
        "train_dataset = train_val_dataset[:num_train_examples]\n",
        "val_dataset = train_val_dataset[num_train_examples:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "predicted_pc_viz = train_dataset[1].pos\n",
        "label = train_dataset[1].y\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(predicted_pc_viz[:, 0],   # x\n",
        "          predicted_pc_viz[:, 1],   # y\n",
        "          predicted_pc_viz[:, 2],   # z\n",
        "          c=label[:], # height data for color\n",
        "          cmap='Spectral',\n",
        "          marker=\"x\")\n",
        "ax.axis('scaled')  # {equal, scaled}\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_8AiGOwvCLNq"
      },
      "id": "_8AiGOwvCLNq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回のタスクは、物体のパーツ単位のセグメンテーションです。\n",
        "したがって、今回のセグメンテーション対象であるギターの各パーツである、ヘッド・ネック・ボディに分かれています。"
      ],
      "metadata": {
        "id": "Tim38_7EAZqk"
      },
      "id": "Tim38_7EAZqk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "データローダーの設定"
      ],
      "metadata": {
        "id": "rzw5SedNw0Mp"
      },
      "id": "rzw5SedNw0Mp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bee5029-3bdd-4fa8-9a37-4d024c0cf6b5",
      "metadata": {
        "id": "8bee5029-3bdd-4fa8-9a37-4d024c0cf6b5"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=config_batch_size,\n",
        "    shuffle=True, num_workers=config_num_workers\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=config_batch_size,\n",
        "    shuffle=False, num_workers=config_num_workers\n",
        ")\n",
        "visualization_loader = DataLoader(\n",
        "    val_dataset[:10], batch_size=1,\n",
        "    shuffle=False, num_workers=config_num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1bc429-7d54-411a-b477-b7bbc7bfc3d6",
      "metadata": {
        "id": "6d1bc429-7d54-411a-b477-b7bbc7bfc3d6"
      },
      "source": [
        "# PyTorch Geometricによる実装"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointNet++を利用した点群のセマンティックセグメンテーションを行います．\n",
        "\n",
        "PointNet++は，入力された点群をダウンサンプリングしながら特徴抽出を行うエンコーダと，ダウンサンプリングされた点群の特徴を元の点群数までアップサンプリングしながら特徴を伝えるデコーダで構成されます．\n",
        "\n",
        "エンコーダでは，Set Abstraction Module(`SAModule`)を適用します．SAModuleでは，入力された点群に対して，最遠点サンプリング（fps）によって入力された点数よりも少ない点数の代表点を抽出します．次に，この代表点に対して特徴量を計算します．特徴計算には代表点の周辺点を半径rの球の中に入る点をグループ化して，このグループ化された点に対してMLPを適用します．その後グループ内の点群\n",
        "に対してMaxPoolingを適用して，代表点に特徴量を与えます．半径これは，小領域に対してPointNetを適用していることと同義です．  \n",
        "なお，点群のグローバルな特徴量を用いると，セグメンテーションの性能が上がるため，一番点数の少なくなった層において，`GlobalSAModule`を用いて，代表点全体の特徴量から代表値を求めます．\n",
        "\n",
        "\n",
        "デコーダでは，サブサンプリングされた点群から元の点群数に戻すために，アップサンプリングを行います．`FPModule`と呼ばれる演算を用いてアップサンプリングを行い，その中身は，knn_interpolate関数を呼び出して，ある点の最近傍の点群に対して特徴量を伝播させます．\n",
        "この際，エンコーダから高解像度の点群情報を用いて，サブサンプリングされる前の高解像度な情報を組みわせます．これは，画像処理におけるセグメンテーションでも行われており，スキップコネクションとも呼ばれます．\n"
      ],
      "metadata": {
        "id": "H22LUEkpFBCH"
      },
      "id": "H22LUEkpFBCH"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
        "class SAModule(torch.nn.Module):\n",
        "    def __init__(self, ratio, r, nn):\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "        self.r = r\n",
        "        self.conv = PointNetConv(nn, add_self_loops=False)\n",
        "\n",
        "    def forward(self, x, pos, batch):\n",
        "        idx = fps(pos, batch, ratio=self.ratio)\n",
        "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
        "                          max_num_neighbors=64)\n",
        "        edge_index = torch.stack([col, row], dim=0)\n",
        "        x_dst = None if x is None else x[idx]\n",
        "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
        "        pos, batch = pos[idx], batch[idx]\n",
        "        return x, pos, batch\n",
        "\n",
        "\n",
        "class GlobalSAModule(torch.nn.Module):\n",
        "    def __init__(self, nn):\n",
        "        super().__init__()\n",
        "        self.nn = nn\n",
        "\n",
        "    def forward(self, x, pos, batch):\n",
        "        x = self.nn(torch.cat([x, pos], dim=1))\n",
        "        x = global_max_pool(x, batch)\n",
        "        pos = pos.new_zeros((x.size(0), 3))\n",
        "        batch = torch.arange(x.size(0), device=batch.device)\n",
        "        return x, pos, batch\n"
      ],
      "metadata": {
        "id": "yZUsn3iuFIsE"
      },
      "id": "yZUsn3iuFIsE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MLP, knn_interpolate\n",
        "from torchmetrics.functional import jaccard_index\n",
        "\n",
        "class FPModule(torch.nn.Module):\n",
        "    def __init__(self, k, nn):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.nn = nn\n",
        "\n",
        "    def forward(self, x, pos, batch, x_skip, pos_skip, batch_skip):\n",
        "        x = knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)\n",
        "        if x_skip is not None:\n",
        "            x = torch.cat([x, x_skip], dim=1)\n",
        "        x = self.nn(x)\n",
        "        return x, pos_skip, batch_skip\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input channels account for both `pos` and node features.\n",
        "        self.sa1_module = SAModule(0.2, 0.2, MLP([3 + 3, 64, 64, 128]))\n",
        "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
        "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
        "\n",
        "        self.fp3_module = FPModule(1, MLP([1024 + 256, 256, 256]))\n",
        "        self.fp2_module = FPModule(3, MLP([256 + 128, 256, 128]))\n",
        "        self.fp1_module = FPModule(3, MLP([128 + 3, 128, 128, 128]))\n",
        "\n",
        "        self.mlp = MLP([128, 128, 128, num_classes], dropout=0.5, norm=None)\n",
        "\n",
        "        self.lin1 = torch.nn.Linear(128, 128)\n",
        "        self.lin2 = torch.nn.Linear(128, 128)\n",
        "        self.lin3 = torch.nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        sa0_out = (data.x, data.pos, data.batch)\n",
        "        sa1_out = self.sa1_module(*sa0_out)\n",
        "        sa2_out = self.sa2_module(*sa1_out)\n",
        "        sa3_out = self.sa3_module(*sa2_out)\n",
        "\n",
        "        fp3_out = self.fp3_module(*sa3_out, *sa2_out)\n",
        "        fp2_out = self.fp2_module(*fp3_out, *sa1_out)\n",
        "        x, _, _ = self.fp1_module(*fp2_out, *sa0_out)\n",
        "\n",
        "        return self.mlp(x).log_softmax(dim=-1)"
      ],
      "metadata": {
        "id": "OjEqTOH4F2oA"
      },
      "id": "OjEqTOH4F2oA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import MLP, knn_interpolate\n",
        "from torchmetrics.functional import jaccard_index\n",
        "config_num_classes = train_dataset.num_classes\n",
        "print(config_num_classes)\n",
        "model = Net(train_dataset.num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = correct_nodes = total_nodes = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct_nodes += out.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'[{i+1}/{len(train_loader)}] Loss: {total_loss / 10:.4f} '\n",
        "                  f'Train Acc: {correct_nodes / total_nodes:.4f}')\n",
        "            total_loss = correct_nodes = total_nodes = 0\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_step(epoch):\n",
        "    model.eval()\n",
        "\n",
        "    ious, categories = [], []\n",
        "    total_loss = correct_nodes = total_nodes = 0\n",
        "    y_map = torch.empty(\n",
        "        val_loader.dataset.num_classes, device=device\n",
        "    ).long()\n",
        "    num_val_examples = len(val_loader)\n",
        "    \n",
        "    progress_bar = tqdm(\n",
        "        val_loader, desc=f\"Validating Epoch {epoch}/{config_epochs}\"\n",
        "    )\n",
        "    \n",
        "    for data in progress_bar:\n",
        "        data = data.to(device)\n",
        "        outs = model(data)\n",
        "        \n",
        "        loss = F.nll_loss(outs, data.y)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "\n",
        "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
        "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
        "                                    data.category.tolist()):\n",
        "            category = list(ShapeNet.seg_classes.keys())[category]\n",
        "            part = ShapeNet.seg_classes[category]\n",
        "            part = torch.tensor(part, device=device)\n",
        "\n",
        "            y_map[part] = torch.arange(part.size(0), device=device)\n",
        "            jaccard_index = MulticlassJaccardIndex(num_classes=part.size(0)).to(device)\n",
        "\n",
        "            iou = jaccard_index(\n",
        "                out[:, part].argmax(dim=-1), y_map[y]\n",
        "            )\n",
        "            ious.append(iou)\n",
        "\n",
        "        categories.append(data.category)\n",
        "\n",
        "    iou = torch.tensor(ious, device=device)\n",
        "    category = torch.cat(categories, dim=0)\n",
        "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
        "    \n",
        "    return {\n",
        "        \"Validation/Loss\": total_loss / num_val_examples,\n",
        "        \"Validation/Accuracy\": correct_nodes / total_nodes,\n",
        "        \"Validation/IoU\": mean_iou\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sNFvqtJVGFDe"
      },
      "id": "sNFvqtJVGFDe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def visualization_step(epoch):\n",
        "    model.eval()\n",
        "    for data in tqdm(visualization_loader):\n",
        "        data = data.to(device)\n",
        "        outs = model(data)\n",
        "\n",
        "        predicted_labels = outs.argmax(dim=1)\n",
        "        accuracy = predicted_labels.eq(data.y).sum().item() / data.num_nodes\n",
        "\n",
        "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
        "        ious, categories = [], []\n",
        "        y_map = torch.empty(\n",
        "            visualization_loader.dataset.num_classes, device=device\n",
        "        ).long()\n",
        "        for out, y, category in zip(\n",
        "            outs.split(sizes), data.y.split(sizes), data.category.tolist()\n",
        "        ):\n",
        "            category = list(ShapeNet.seg_classes.keys())[category]\n",
        "            part = ShapeNet.seg_classes[category]\n",
        "            part = torch.tensor(part, device=device)\n",
        "            y_map[part] = torch.arange(part.size(0), device=device)\n",
        "            jaccard_index = MulticlassJaccardIndex(num_classes=part.size(0)).to(device)\n",
        "            iou = jaccard_index(\n",
        "                out[:, part].argmax(dim=-1), y_map[y]\n",
        "            )\n",
        "            ious.append(iou)\n",
        "        categories.append(data.category)\n",
        "        iou = torch.tensor(ious, device=device)\n",
        "        category = torch.cat(categories, dim=0)\n",
        "        mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
        "\n",
        "        gt_pc_viz = data.pos.cpu().numpy().tolist()\n",
        "        segmentation_label = data.y.cpu().numpy().tolist()\n",
        "        predicted_labels =  predicted_labels.cpu().numpy().tolist()\n",
        "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
        "        for label in set(segmentation_label):\n",
        "            frequency_dict[label] = segmentation_label.count(label)\n",
        "        for j in range(len(gt_pc_viz)):\n",
        "            # gt_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
        "            gt_pc_viz[j] += [segmentation_label[j] + 1]\n",
        "\n",
        "        predicted_pc_viz = data.pos.cpu().numpy().tolist()\n",
        "        segmentation_label = data.y.cpu().numpy().tolist()\n",
        "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
        "        for label in set(segmentation_label):\n",
        "            frequency_dict[label] = segmentation_label.count(label)\n",
        "        for j in range(len(predicted_pc_viz)):\n",
        "            # predicted_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
        "            predicted_pc_viz[j] += [predicted_labels[j] + 1]\n",
        "\n",
        "    \n",
        "    return predicted_pc_viz"
      ],
      "metadata": {
        "id": "P1OeZ3fBGybz"
      },
      "id": "P1OeZ3fBGybz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=config_lr_scheduler_step_size, gamma=config_gamma\n",
        ")\n",
        "for epoch in range(1, config_epochs + 1):\n",
        "    predicted_pc_viz = visualization_step(epoch)\n",
        "    predicted_pc_viz = np.array(predicted_pc_viz)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train()\n",
        "    val_metrics = val_step(epoch)\n",
        "    \n",
        "    metrics = {**val_metrics}\n",
        "    print(metrics)\n",
        "    metrics[\"learning_rate\"] = scheduler.get_last_lr()[-1]\n",
        "\n",
        "\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    ax.scatter(predicted_pc_viz[:, 0],   # x\n",
        "              predicted_pc_viz[:, 1],   # y\n",
        "              predicted_pc_viz[:, 2],   # z\n",
        "              c=predicted_pc_viz[:, 3], # height data for color\n",
        "              cmap='Spectral',\n",
        "              marker=\"x\")\n",
        "    ax.axis('scaled')  # {equal, scaled}\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oWFDCep8IPA0"
      },
      "id": "oWFDCep8IPA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回はDGCNNという点群の深層学習手法を実装します。\n",
        "DGCNNはDynamic Graph CNNという手法で、各点に対して特徴量空間のKNNで周辺の点を検索し、そのKNNの点に対してDynamicEdgeConvを適用します。\n",
        "\n",
        "\n",
        "\n",
        "層の構造は、各点に対して`DynamicEdgeConv`を適用します。\n",
        "このDynamicEdgeConvを3回くりかえして、周辺点の情報を集約していきます。\n",
        "その後、最終層で各点ごとにクラス分類を行います。\n",
        "クラス分類には、単純なＭＬＰを適用し、出力次元数は分類したいクラス数と同じにします。\n",
        "この実装では、返り値を計算する際に、`F.log_softmax`を適用します。"
      ],
      "metadata": {
        "id": "CJsNI93Siuz0"
      },
      "id": "CJsNI93Siuz0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1bd7a8-585f-45fc-bc59-a8feec3512db",
      "metadata": {
        "id": "ac1bd7a8-585f-45fc-bc59-a8feec3512db"
      },
      "outputs": [],
      "source": [
        "class DGCNN(torch.nn.Module):\n",
        "    def __init__(self, out_channels, k=30, aggr='max'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = DynamicEdgeConv(MLP([2 * 6, 64, 64]), k, aggr)\n",
        "        self.conv2 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
        "        self.conv3 = DynamicEdgeConv(MLP([2 * 64, 64, 64]), k, aggr)\n",
        "\n",
        "        self.mlp = MLP(\n",
        "            [3 * 64, 1024, 256, 128, out_channels],\n",
        "            dropout=0.5, norm=None\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, pos, batch = data.x, data.pos, data.batch\n",
        "        x0 = torch.cat([x, pos], dim=-1)\n",
        "        \n",
        "        x1 = self.conv1(x0, batch)\n",
        "        x2 = self.conv2(x1, batch)\n",
        "        x3 = self.conv3(x2, batch)\n",
        "        \n",
        "        out = self.mlp(torch.cat([x1, x2, x3], dim=1))\n",
        "        return F.log_softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの呼び出し"
      ],
      "metadata": {
        "id": "OgcpCYeNxCi3"
      },
      "id": "OgcpCYeNxCi3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a677e7b-9179-4c71-b1dc-af636dcf95d5",
      "metadata": {
        "id": "5a677e7b-9179-4c71-b1dc-af636dcf95d5"
      },
      "outputs": [],
      "source": [
        "config_num_classes = train_dataset.num_classes\n",
        "print(config_num_classes)\n",
        "model = DGCNN(\n",
        "    out_channels=train_dataset.num_classes,\n",
        "    k=config_num_nearest_neighbours,\n",
        "    aggr=config_aggregation_operator\n",
        ").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config_initial_lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=config_lr_scheduler_step_size, gamma=config_gamma\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d94603b-51df-4299-ae69-f43617827d1c",
      "metadata": {
        "id": "3d94603b-51df-4299-ae69-f43617827d1c"
      },
      "source": [
        "# 訓練の実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9fbe02-b095-4df7-ab6b-af0dd6a411bf",
      "metadata": {
        "id": "1a9fbe02-b095-4df7-ab6b-af0dd6a411bf"
      },
      "outputs": [],
      "source": [
        "def train_step(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    ious, categories = [], []\n",
        "    total_loss = correct_nodes = total_nodes = 0\n",
        "    y_map = torch.empty(\n",
        "        train_loader.dataset.num_classes, device=device\n",
        "    ).long()\n",
        "    num_train_examples = len(train_loader)\n",
        "    \n",
        "    progress_bar = tqdm(\n",
        "        train_loader, desc=f\"Training Epoch {epoch}/{config_epochs}\"\n",
        "    )\n",
        "    \n",
        "    for data in progress_bar:\n",
        "        data = data.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outs = model(data)\n",
        "        loss = F.nll_loss(outs, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "        \n",
        "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
        "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
        "                                    data.category.tolist()):\n",
        "            category = list(ShapeNet.seg_classes.keys())[category]\n",
        "            part = ShapeNet.seg_classes[category]\n",
        "            part = torch.tensor(part, device=device)\n",
        "\n",
        "            y_map[part] = torch.arange(part.size(0), device=device)\n",
        "\n",
        "            jaccard_index = MulticlassJaccardIndex(num_classes=part.size(0)).to(device)\n",
        "\n",
        "            iou = jaccard_index(\n",
        "                out[:, part].argmax(dim=-1), y_map[y]\n",
        "            )\n",
        "            ious.append(iou)\n",
        "\n",
        "        categories.append(data.category)\n",
        "        \n",
        "    iou = torch.tensor(ious, device=device)\n",
        "    category = torch.cat(categories, dim=0)\n",
        "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
        "    \n",
        "    return {\n",
        "        \"Train/Loss\": total_loss / num_train_examples,\n",
        "        \"Train/Accuracy\": correct_nodes / total_nodes,\n",
        "        \"Train/IoU\": mean_iou\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d7e550-19b1-40b8-9e01-2997a694a4fa",
      "metadata": {
        "id": "94d7e550-19b1-40b8-9e01-2997a694a4fa"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def val_step(epoch):\n",
        "    model.eval()\n",
        "\n",
        "    ious, categories = [], []\n",
        "    total_loss = correct_nodes = total_nodes = 0\n",
        "    y_map = torch.empty(\n",
        "        val_loader.dataset.num_classes, device=device\n",
        "    ).long()\n",
        "    num_val_examples = len(val_loader)\n",
        "    \n",
        "    progress_bar = tqdm(\n",
        "        val_loader, desc=f\"Validating Epoch {epoch}/{config_epochs}\"\n",
        "    )\n",
        "    \n",
        "    for data in progress_bar:\n",
        "        data = data.to(device)\n",
        "        outs = model(data)\n",
        "        \n",
        "        loss = F.nll_loss(outs, data.y)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        correct_nodes += outs.argmax(dim=1).eq(data.y).sum().item()\n",
        "        total_nodes += data.num_nodes\n",
        "\n",
        "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
        "        for out, y, category in zip(outs.split(sizes), data.y.split(sizes),\n",
        "                                    data.category.tolist()):\n",
        "            category = list(ShapeNet.seg_classes.keys())[category]\n",
        "            part = ShapeNet.seg_classes[category]\n",
        "            part = torch.tensor(part, device=device)\n",
        "\n",
        "            y_map[part] = torch.arange(part.size(0), device=device)\n",
        "            jaccard_index = MulticlassJaccardIndex(num_classes=part.size(0)).to(device)\n",
        "\n",
        "            iou = jaccard_index(\n",
        "                out[:, part].argmax(dim=-1), y_map[y]\n",
        "            )\n",
        "            ious.append(iou)\n",
        "\n",
        "        categories.append(data.category)\n",
        "\n",
        "    iou = torch.tensor(ious, device=device)\n",
        "    category = torch.cat(categories, dim=0)\n",
        "    mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
        "    \n",
        "    return {\n",
        "        \"Validation/Loss\": total_loss / num_val_examples,\n",
        "        \"Validation/Accuracy\": correct_nodes / total_nodes,\n",
        "        \"Validation/IoU\": mean_iou\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果を取得する関数"
      ],
      "metadata": {
        "id": "1oNiu4MPxaj3"
      },
      "id": "1oNiu4MPxaj3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4fad76-f961-4a0a-9f37-c72c1422e885",
      "metadata": {
        "id": "da4fad76-f961-4a0a-9f37-c72c1422e885"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def visualization_step(epoch):\n",
        "    model.eval()\n",
        "    for data in tqdm(visualization_loader):\n",
        "        data = data.to(device)\n",
        "        outs = model(data)\n",
        "\n",
        "        predicted_labels = outs.argmax(dim=1)\n",
        "        accuracy = predicted_labels.eq(data.y).sum().item() / data.num_nodes\n",
        "\n",
        "        sizes = (data.ptr[1:] - data.ptr[:-1]).tolist()\n",
        "        ious, categories = [], []\n",
        "        y_map = torch.empty(\n",
        "            visualization_loader.dataset.num_classes, device=device\n",
        "        ).long()\n",
        "        for out, y, category in zip(\n",
        "            outs.split(sizes), data.y.split(sizes), data.category.tolist()\n",
        "        ):\n",
        "            category = list(ShapeNet.seg_classes.keys())[category]\n",
        "            part = ShapeNet.seg_classes[category]\n",
        "            part = torch.tensor(part, device=device)\n",
        "            y_map[part] = torch.arange(part.size(0), device=device)\n",
        "            jaccard_index = MulticlassJaccardIndex(num_classes=part.size(0)).to(device)\n",
        "            iou = jaccard_index(\n",
        "                out[:, part].argmax(dim=-1), y_map[y]\n",
        "            )\n",
        "            ious.append(iou)\n",
        "        categories.append(data.category)\n",
        "        iou = torch.tensor(ious, device=device)\n",
        "        category = torch.cat(categories, dim=0)\n",
        "        mean_iou = float(scatter(iou, category, reduce='mean').mean())\n",
        "\n",
        "        gt_pc_viz = data.pos.cpu().numpy().tolist()\n",
        "        segmentation_label = data.y.cpu().numpy().tolist()\n",
        "        predicted_labels =  predicted_labels.cpu().numpy().tolist()\n",
        "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
        "        for label in set(segmentation_label):\n",
        "            frequency_dict[label] = segmentation_label.count(label)\n",
        "        for j in range(len(gt_pc_viz)):\n",
        "            # gt_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
        "            gt_pc_viz[j] += [segmentation_label[j] + 1]\n",
        "\n",
        "        predicted_pc_viz = data.pos.cpu().numpy().tolist()\n",
        "        segmentation_label = data.y.cpu().numpy().tolist()\n",
        "        frequency_dict = {key: 0 for key in segmentation_class_frequency.keys()}\n",
        "        for label in set(segmentation_label):\n",
        "            frequency_dict[label] = segmentation_label.count(label)\n",
        "        for j in range(len(predicted_pc_viz)):\n",
        "            # predicted_pc_viz[j] += [segmentation_label[j] + 1 - class_offset]\n",
        "            predicted_pc_viz[j] += [predicted_labels[j] + 1]\n",
        "\n",
        "    \n",
        "    return predicted_pc_viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c31a63-96a4-409f-bb0d-2b56b034adf0",
      "metadata": {
        "id": "e8c31a63-96a4-409f-bb0d-2b56b034adf0"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch):\n",
        "    \"\"\"Save model checkpoints as Weights & Biases artifacts\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, \"checkpoint.pt\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際の訓練を回してみましょう。\n",
        "今回は1エポックですが、`config_epochs`の値を大きくすると学習する回数が多くなります。"
      ],
      "metadata": {
        "id": "Seixq_0XAM69"
      },
      "id": "Seixq_0XAM69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f013430-fd25-4541-9dd3-3c67eef9074a",
      "metadata": {
        "id": "6f013430-fd25-4541-9dd3-3c67eef9074a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "for epoch in range(1, config_epochs + 1):\n",
        "    predicted_pc_viz = visualization_step(epoch)\n",
        "    predicted_pc_viz = np.array(predicted_pc_viz)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train_metrics = train_step(epoch)\n",
        "    val_metrics = val_step(epoch)\n",
        "    \n",
        "    metrics = {**train_metrics, **val_metrics}\n",
        "    print(metrics)\n",
        "    metrics[\"learning_rate\"] = scheduler.get_last_lr()[-1]\n",
        "\n",
        "\n",
        "    \n",
        "    scheduler.step()\n",
        "    save_checkpoint(epoch)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    ax.scatter(predicted_pc_viz[:, 0],   # x\n",
        "              predicted_pc_viz[:, 1],   # y\n",
        "              predicted_pc_viz[:, 2],   # z\n",
        "              c=predicted_pc_viz[:, 3], # height data for color\n",
        "              cmap='Spectral',\n",
        "              marker=\"x\")\n",
        "    ax.axis('scaled')  # {equal, scaled}\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習回数が1エポックなので、あまり学習が進んでおわず、パーツ単位のセグメンテーションがうまくいっていないことがわかると思います。今回の講義では、時間の制約で1エポックとしていますが、エポック数を長く設定するともう少しセグメンテーションの結果が良くなります。"
      ],
      "metadata": {
        "id": "fcHWbwG0AFgR"
      },
      "id": "fcHWbwG0AFgR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}