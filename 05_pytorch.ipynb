{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shnhrtkyk/JTCcode/blob/main/05_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorchの使い方"
      ],
      "metadata": {
        "id": "DOG79uVBvPe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "講習では、Pytorchという深層学習ライブラリを使用してモデルの構築や学習を行います。\n",
        "まずは、簡単な処理を構築してみましょう。"
      ],
      "metadata": {
        "id": "MjsrNchhvR02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aNgmJ3pfdUpy",
        "outputId": "efce3b4d-13f1-4deb-ce9c-b500a3b936f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# 線形変換を行う関数\n",
        "linear = nn.Linear(128, 256)\n",
        "# 入力データを作成\n",
        "input_data = torch.zeros((128))\n",
        "\n",
        "x = linear(input_data)\n",
        "# 活性化関数を適用\n",
        "x = F.relu(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 簡単なマルチレイヤーパーセプトロンの実装"
      ],
      "metadata": {
        "id": "Hy7lHnk9yKq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "マルチレイヤーパーセプトロンは全結合層で成り立つニューラルネットワークです。\n",
        "Pytorchで実装する際には、まず使いたい関数を`__init__`で定義します。\n",
        "その後、実際に順計算するのは`forward`の中です。\n"
      ],
      "metadata": {
        "id": "yOFgLbAoyOTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8uxl6Vj6dUpz",
        "outputId": "0df3d9e0-ca3c-4de8-c6f0-fa66b76467f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        # 線形変換を行う全結合層を定義\n",
        "        self.linear1 = nn.Linear(128, 256)\n",
        "        # 線形変換を行う全結合層を定義\n",
        "        self.linear2 = nn.Linear(256, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x) # まず、全結合層へ入力する\n",
        "        x = F.relu(x) # 次に活性化関数を適用する\n",
        "        x = self.linear2(x) # 最後に全結合層へ入力する\n",
        "        return x\n",
        "# MLPという名前で定義した層設計のクラスをnetという名前で呼び出します。\n",
        "net = MLP()\n",
        "# netにinput_dataを入力する\n",
        "y = net(input_data)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "そのほかにも層設計の定義方法はあり`nn.Sequential`で同様の処理を実現できます。"
      ],
      "metadata": {
        "id": "_2fUsc2tzwv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FsVkOEpjdUp0",
        "outputId": "23e3aa78-1268-4a29-c4be-5b6349800dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "net = nn.Sequential(nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 10))\n",
        "y = net(input_data)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "また、入力する次元は、バッチと呼ばれる一度にモデルへ入力することができるひとまとまりごとに入力できます。\n",
        "以下の例では、32個のデータを一気に深層学習モデルへ入力します。"
      ],
      "metadata": {
        "id": "80AU9ZTtz_Tq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8NEYDRtgdUp0",
        "outputId": "ff3bc5cd-13d3-4846-c015-afd0536f3195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10])\n"
          ]
        }
      ],
      "source": [
        "input_data = torch.rand([32, 128])\n",
        "y = net(input_data)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習のやりかた"
      ],
      "metadata": {
        "id": "bTMx5Jw40qps"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPkyI-DJ38-R"
      },
      "source": [
        "パラメータの最適化\n",
        "===========================\n",
        "\n",
        "モデルとデータを用意できたので続いてはモデルを訓練、検証することで、データに対してモデルのパラメータを最適化し、テストを行います。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow4ZimTD48XR"
      },
      "source": [
        "モデルの訓練は反復的なプロセスとなります。\n",
        "\n",
        "各イテレーション（エポックと呼ばれます）で、モデルは出力を計算し、損失を求めます。そして各パラメータについて損失に対する偏微分の値を求めます。\n",
        "\n",
        "その後、勾配降下法に基づいてパラメータを最適化します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz96ICKoEgie"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0HPbxJy480B"
      },
      "source": [
        "## 層設計など \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-QFvrJl38-L"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNpggd8W38-R"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PidFXE1LEjR1"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeozEcms38-S"
      },
      "source": [
        "ハイパーパラメータ\n",
        "-----------------\n",
        "\n",
        "ハイパーパラメータは、モデルの最適化プロセスを制御するためのパラメータです。\n",
        "\n",
        "ハイパーパラメータの値が異なると、モデルの学習や収束率に影響します（詳細なハイパーパラメータチューニングの解説は[こちら](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)をご覧ください）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XWChnhIEtHH"
      },
      "source": [
        "\n",
        "今回は、訓練用のハイパーパラメータとして以下の値を使用します。\n",
        "\n",
        " - **Number of Epochs**：イテレーション回数\n",
        " - **Batch Size**：ミニバッチサイズを構成するデータ数\n",
        " - **Learning Rate**：パラメータ更新の係数。値が小さいと変化が少なく、大きすぎると訓練に失敗する可能性が生まれる\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpay3IJz38-U"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW8AYLUtExtf"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1bOwA_38-V"
      },
      "source": [
        "最適化ループ\n",
        "-----------------\n",
        "ハイパーパラメータを設定後、訓練で最適化のループを回すことで、モデルを最適化します。\n",
        "\n",
        "最適化ループの1回のイテレーションは、**エポック(epoch)**と呼ばれます。\n",
        "\n",
        "各エポックでは2種類のループから構成されます。\n",
        "\n",
        " - **訓練ループ**：データセットに対して訓練を実行し、パラメータを収束させます\n",
        " \n",
        " - **検証 / テストループ**：テストデータセットでモデルを評価し、性能が向上しているか確認します\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8_uj3QLLxvT"
      },
      "source": [
        "訓練ループ内で使用される概念について、簡単に把握しておきましょう。\n",
        "\n",
        "本チュートリアルの最後には、最適化ループの完全な実装を紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7JihOQJLx4B"
      },
      "source": [
        "**損失関数：Loss Function**\n",
        "\n",
        "データが与えられても、訓練されていないネットワークは正しい答えを出力しない可能性があります。\n",
        " \n",
        "損失関数はモデルが推論した結果と、実際の正解との誤差の大きさを測定する関数です。訓練ではこの損失関数の値を小さくしていきます。\n",
        "\n",
        "損失を計算するためには、入力データに対するモデルの推論結果を求め、その値と正解のラベルとの違いを比較します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbuOTl6wNm-7"
      },
      "source": [
        "一般的な損失関数としては、回帰タスクでは[`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)(Mean Square Error)、分類タスクでは[`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)(Negative Log Likelihood) が使用されます。\n",
        "\n",
        "[`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)は、``nn.LogSoftmax`` と ``nn.NLLLoss``を結合した損失関数となります。\n",
        "\n",
        "モデルが出力する`logit`値を`nn.CrossEntropyLoss`に与えて正規化し、予測誤差を求めます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayTh0SOU38-W"
      },
      "source": [
        "# loss functionの初期化、定義\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEUFRDx638-W"
      },
      "source": [
        "**最適化器：Optimizer**\n",
        "\n",
        "最適化は各訓練ステップにおいてモデルの誤差を小さくなるように、モデルパラメータを調整するプロセスです。\n",
        "\n",
        "<br>\n",
        "\n",
        "**最適化アルゴリズム：Optimization algorithms**\n",
        "\n",
        "最適化アルゴリズムは、最適化プロセスの具体的な手続きです（本チュートリアルでは確率的勾配降下法：Stochastic Gradient Descentを使用します）。\n",
        "\n",
        "最適化のロジックは全て``optimizer``オブジェクト内に隠ぺいされます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB8RLaw3PQ3Z"
      },
      "source": [
        " 今回はSGD optimizerを使用します。ただし、最適化関数にはADAMやRMSPropなど、様々な種類があります。\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMwa7PPmFzjv"
      },
      "source": [
        "\n",
        "訓練したいモデルパラメータをoptimizerに登録し、合わせて学習率をハイパーパラメータとして渡すことで初期化を行います。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIH9ERrX38-X"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c53YLpx038-Y"
      },
      "source": [
        "訓練ループ内で、最適化（optimization）は3つのステップから構成されます。\n",
        "\n",
        "[1] ``optimizer.zero_grad()``を実行し、モデルパラメータの勾配をリセットします。\n",
        "\n",
        "勾配の計算は蓄積されていくので、毎イテレーション、明示的にリセットします。\n",
        "\n",
        "<br>\n",
        "\n",
        "[2] 続いて、``loss.backwards()``を実行し、バックプロパゲーションを実行します。\n",
        "\n",
        "PyTorchは損失に対する各パラメータの偏微分の値（勾配）を求めます。\n",
        "\n",
        "<br>\n",
        "\n",
        "[3] 最後に、``optimizer.step()``を実行し、各パラメータの勾配を使用してパラメータの値を調整します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4vSCUCEGBEz"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulov18zc38-Y"
      },
      "source": [
        "実装全体：Full Implementation\n",
        "-----------------------\n",
        "最適化を実行するコードをループする``train_loop``と、テストデータに対してモデルの性能を評価する``test_loop``を定義します。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVvdppwu38-Z"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):        \n",
        "        # 予測と損失の計算\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pq-tYFz38-Z"
      },
      "source": [
        "損失関数とoptimizerを初期化し、それを ``train_loop`` と ``test_loop`` に渡します。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhpe8Lia38-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1b2ba9-508d-4ba9-a2c1-b50cc2bd20a1"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298138  [    0/60000]\n",
            "loss: 2.294968  [ 6400/60000]\n",
            "loss: 2.277383  [12800/60000]\n",
            "loss: 2.285448  [19200/60000]\n",
            "loss: 2.280416  [25600/60000]\n",
            "loss: 2.242236  [32000/60000]\n",
            "loss: 2.273299  [38400/60000]\n",
            "loss: 2.239137  [44800/60000]\n",
            "loss: 2.252436  [51200/60000]\n",
            "loss: 2.231144  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 26.5%, Avg loss: 0.034983 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.225547  [    0/60000]\n",
            "loss: 2.226835  [ 6400/60000]\n",
            "loss: 2.178338  [12800/60000]\n",
            "loss: 2.214661  [19200/60000]\n",
            "loss: 2.201105  [25600/60000]\n",
            "loss: 2.128296  [32000/60000]\n",
            "loss: 2.204439  [38400/60000]\n",
            "loss: 2.131501  [44800/60000]\n",
            "loss: 2.170639  [51200/60000]\n",
            "loss: 2.139056  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 35.6%, Avg loss: 0.033286 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.117138  [    0/60000]\n",
            "loss: 2.111336  [ 6400/60000]\n",
            "loss: 2.015203  [12800/60000]\n",
            "loss: 2.103530  [19200/60000]\n",
            "loss: 2.064550  [25600/60000]\n",
            "loss: 1.952058  [32000/60000]\n",
            "loss: 2.099865  [38400/60000]\n",
            "loss: 1.969500  [44800/60000]\n",
            "loss: 2.054186  [51200/60000]\n",
            "loss: 2.018766  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.8%, Avg loss: 0.030986 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.965862  [    0/60000]\n",
            "loss: 1.957449  [ 6400/60000]\n",
            "loss: 1.806781  [12800/60000]\n",
            "loss: 1.969279  [19200/60000]\n",
            "loss: 1.917930  [25600/60000]\n",
            "loss: 1.772560  [32000/60000]\n",
            "loss: 2.003124  [38400/60000]\n",
            "loss: 1.822496  [44800/60000]\n",
            "loss: 1.938400  [51200/60000]\n",
            "loss: 1.928293  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 42.6%, Avg loss: 0.028981 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.824601  [    0/60000]\n",
            "loss: 1.824758  [ 6400/60000]\n",
            "loss: 1.630993  [12800/60000]\n",
            "loss: 1.858709  [19200/60000]\n",
            "loss: 1.806184  [25600/60000]\n",
            "loss: 1.647794  [32000/60000]\n",
            "loss: 1.928116  [38400/60000]\n",
            "loss: 1.721758  [44800/60000]\n",
            "loss: 1.845744  [51200/60000]\n",
            "loss: 1.863042  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.9%, Avg loss: 0.027498 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.716815  [    0/60000]\n",
            "loss: 1.728603  [ 6400/60000]\n",
            "loss: 1.505531  [12800/60000]\n",
            "loss: 1.776900  [19200/60000]\n",
            "loss: 1.718424  [25600/60000]\n",
            "loss: 1.562549  [32000/60000]\n",
            "loss: 1.868672  [38400/60000]\n",
            "loss: 1.651557  [44800/60000]\n",
            "loss: 1.774211  [51200/60000]\n",
            "loss: 1.810434  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.3%, Avg loss: 0.026379 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.635293  [    0/60000]\n",
            "loss: 1.660866  [ 6400/60000]\n",
            "loss: 1.414392  [12800/60000]\n",
            "loss: 1.694878  [19200/60000]\n",
            "loss: 1.626909  [25600/60000]\n",
            "loss: 1.482292  [32000/60000]\n",
            "loss: 1.773832  [38400/60000]\n",
            "loss: 1.556403  [44800/60000]\n",
            "loss: 1.670701  [51200/60000]\n",
            "loss: 1.692423  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 47.5%, Avg loss: 0.024471 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.555340  [    0/60000]\n",
            "loss: 1.589365  [ 6400/60000]\n",
            "loss: 1.286841  [12800/60000]\n",
            "loss: 1.543253  [19200/60000]\n",
            "loss: 1.526771  [25600/60000]\n",
            "loss: 1.401455  [32000/60000]\n",
            "loss: 1.675433  [38400/60000]\n",
            "loss: 1.472257  [44800/60000]\n",
            "loss: 1.589028  [51200/60000]\n",
            "loss: 1.606701  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.6%, Avg loss: 0.023059 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.487669  [    0/60000]\n",
            "loss: 1.534931  [ 6400/60000]\n",
            "loss: 1.197659  [12800/60000]\n",
            "loss: 1.449077  [19200/60000]\n",
            "loss: 1.456404  [25600/60000]\n",
            "loss: 1.333213  [32000/60000]\n",
            "loss: 1.602819  [38400/60000]\n",
            "loss: 1.407391  [44800/60000]\n",
            "loss: 1.521342  [51200/60000]\n",
            "loss: 1.541975  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 0.021951 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.424136  [    0/60000]\n",
            "loss: 1.485925  [ 6400/60000]\n",
            "loss: 1.125033  [12800/60000]\n",
            "loss: 1.377324  [19200/60000]\n",
            "loss: 1.393583  [25600/60000]\n",
            "loss: 1.271893  [32000/60000]\n",
            "loss: 1.538753  [38400/60000]\n",
            "loss: 1.350104  [44800/60000]\n",
            "loss: 1.459396  [51200/60000]\n",
            "loss: 1.487531  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.2%, Avg loss: 0.021013 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}